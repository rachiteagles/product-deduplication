{"cells":[{"cell_type":"markdown","source":["Name - Rachit Yadav\n\nRole - Fetch the data from mongo db and preprocess it to feed it to Neural Network\n\nAlgorithim - Neural Network\n\n\n\nName - Charudatta Manwatkar\n\nRole - Develop the model and Hyperparameter tuning\n\nAlgorithim - Neural Network"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79f71ffd-b4a8-4125-a4f4-16cfcaa73b6d"}}},{"cell_type":"markdown","source":["This notebook connects mongo db to fetch embeddings stored on various collections. Each collection consists of embeddings stored generated using different pretrained models. Pretrained models used are-\n1. vgg-11 (collection name - Images (embedding size - 1000), image_4 (embedding size - 4096))\n2. Resnet-18 (collection name - image_2 (embedding size - 512))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc46019f-75bf-416b-bd7a-331d47654829"}}},{"cell_type":"code","source":["from pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"243ed441-7fd7-4cc0-aa94-e5c07b7ef8d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom datetime import datetime\nfrom pyspark.sql.window import Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55fe4636-8dd3-40b2-9feb-56de1e302918"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Setting up configurations\nsc1 = SparkSession.builder.appName('group3')\\\n    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.0\")\\\n    .config(\"spark.network.timeout\", \"36000000s\")\\\n    .config(\"spark.executor.heartbeatInterval\", \"3600s\")\\\n    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://root:root@cluster0.eq07a.mongodb.net/Group3.image_4\")\\\n    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://root:root@cluster0.eq07a.mongodb.net/Group3.image_4\")\\\n    .config(\"spark.databricks.io.cache.enabled\", \"true\")\\\n    .config(\"spark.network.timeout\", \"7200s\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9e9bbf0-e02d-44a5-a8c8-0c8af9b044fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparkContext = sc1.sparkContext\nsparkContext.setLogLevel('OFF')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cb0f357-38a3-4503-a384-d698794c77e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Below mentioned code is used to connect to different collections present on Mongo-db. Just assign collection name to the collection variable"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edcc040-7afb-43d2-b81e-6a1e2b902dc8"}}},{"cell_type":"code","source":["database = 'Group3'\ncollection = 'image_4'\nuser_name = 'root'\npassword = 'root'\naddress = 'cluster0.eq07a.mongodb.net'\nconnection_string = f\"mongodb+srv://{user_name}:{password}@{address}/{database}.{collection}\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b4289b1-e0c3-486a-82f2-06a2cfd83177"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# connects to the database to fetch the data that consists of image name and it's corresponding embedding using collection_string\ndff = spark.read.format(\"mongo\").option(\"uri\",connection_string).load().cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c82831e-85ea-4ca6-b741-e9f0dad4db7a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Below code connects to the mongo database to fetch train_csv file. This file has columns posting_id, image, image_phash, title, label_group. Among these we use image and label_group only."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48543a88-37db-4112-95cd-85d705c5f4b7"}}},{"cell_type":"code","source":["database = 'Group3'\ncollection = 'train_csv'\nuser_name = 'root'\npassword = 'root'\naddress = 'cluster0.eq07a.mongodb.net'\nconnection_string = f\"mongodb+srv://{user_name}:{password}@{address}/{database}.{collection}\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44b8520d-aa77-4c80-93a0-6b87224f0ae4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["lkp=spark.read.format(\"mongo\").option(\"uri\",connection_string).load().select('image','label_group').cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3b13edd-d2bb-4b5c-b91b-c52864b3c609"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We only choose those label groups that counts greater than 5."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f57cfe75-4476-4382-9365-766a71779aa4"}}},{"cell_type":"code","source":["rel_labels = lkp.groupby('label_group').count().filter(col('count')>5).select('label_group')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a98ceb8-d073-4fd8-9666-803ec8962dbb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#lkp1 has all the image file name and label name whose count is greater than 5. Repartition the lkp1 data to 4 partitions as doing so imporoved the performance\nlkp1 = lkp.join(rel_labels,'label_group').repartition(4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8facd01b-5404-4cfe-9de1-b93f53d5cc28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Cross join lkp1 dataframe so that we can have combination of all the matching and non matching pairs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76e47149-7d09-4b49-88de-3aa79dc7104c"}}},{"cell_type":"code","source":["cross_j = lkp1.crossJoin(lkp1).sample(fraction=0.9,seed=3).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a217a7fe-3315-4bcd-b779-4cc1f8583498"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cross_j.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3752474-b0c9-4596-9e83-aac426fd9007"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[12]: 16</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: 16</div>"]}}],"execution_count":0},{"cell_type":"code","source":["newcolumns = [\n 'label_code1',\n'image1',\n \n 'label_code2',\n'image2',]\n\nresult_df_ren=cross_j.toDF(*newcolumns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e756dcf-3f18-4bef-aa67-d7f87bb3eff6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import functions as f\n\ncresult_df_add=result_df_ren.withColumn('binary_label',f.when((result_df_ren.label_code1== result_df_ren.label_code2), '1').otherwise('0') ).select('image1','image2','binary_label').cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74911dba-d0dc-4048-b1c3-cc2e5c2c4838"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cresult_df_add.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbddfb99-9338-480d-abc6-cb97be065b1f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[15]: 16</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: 16</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Since after cross join, we have more matching label group pairs rather than non matching label groups. Therefore, below we downsample the negative data points"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12c53239-71ad-4cb0-920c-5bc836dfc515"}}},{"cell_type":"code","source":["cresult_df_pos = cresult_df_add.filter(col('binary_label')==1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4576c95-891f-48c8-b06d-995ccadc4c34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cnt_pos = cresult_df_pos.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be7e7f13-4ec4-4df4-bf2a-4afe7e1286fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cresult_df_neg = cresult_df_add.filter(col('binary_label')==0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd0babe6-7672-4b1d-8aae-2d65b0f873eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["cnt_neg = cresult_df_neg.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3b41c30-f600-4a6c-a805-cf5cdeb480bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#random sample the negative datapoints using the ratio of positive count to the negative count\ncresult_df_neg_red = cresult_df_neg.sample(withReplacement=False,fraction=cnt_pos/cnt_neg).limit(cnt_pos)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fcb8e26-4c2e-4ba0-81c3-a1d1e82614c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# combined the positive dataframe and negative dataframe\ndata = cresult_df_pos.union(cresult_df_neg_red).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a95c2c80-aebe-4d3f-82e3-88501d582247"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#extract file name from the path\ndff_new = dff.rdd.map(lambda x:[x[0].split('/')[-1],x[1],x[2]]).toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7bc718-8cdc-4d64-9793-76439099e9d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["newcolumns = [ 'image1', 'array']\n\ndff_new_wid_col=dff_new.select('_1','_2').toDF(*newcolumns).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4eca6e20-4fe0-4230-8182-3b25c5e281b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# this function fetches the embedding for given image file name\ndef conv(file):\n    \n    return dff_new_wid_col.filter(col('image1')==file).select('array').collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d2c1c60-1674-43e6-8fb4-e057e6eefd54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Modelling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e8b9c07-26c8-4ef2-8eee-d87961f54705"}}},{"cell_type":"code","source":["import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb1658d8-14bb-4d7d-a28b-1193a56dd128"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[25]: device(type=&#39;cuda&#39;)</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: device(type=&#39;cuda&#39;)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Initializing the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2677e638-5a13-4ce7-94c6-4fb6888eaa56"}}},{"cell_type":"code","source":["%%time\n\n# import torch\nimport numpy as np\nfrom torch import nn\n\nmodel = nn.Sequential(nn.Linear(4096*2, 4096),\n                      nn.BatchNorm1d(4096),\n                      nn.LeakyReLU(0.1),\n                      nn.Linear(4096, 4096//2),\n                      nn.BatchNorm1d(4096//2),\n                      nn.ReLU(),\n                      nn.Linear(4096//2,1)).to(device)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea0c6568-2f09-49eb-a99e-9e0465d0b022"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">CPU times: user 3.51 s, sys: 2.8 s, total: 6.3 s\nWall time: 9.58 s\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CPU times: user 3.51 s, sys: 2.8 s, total: 6.3 s\nWall time: 9.58 s\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["criterion = nn.BCEWithLogitsLoss()\n# batch_size = 8\niteration= 20\ntotal = data.count()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a10d70d5-a1e8-4bc5-b3c0-fdeac7ad27d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# train test split\ns_data = data.randomSplit([0.8, 0.2])\ntrain_data = s_data[0].cache()\ntest_data = s_data[1].cache()\ntrain_loss_lst = []\ntest_loss_lst = []\ntrain_accuracy_lst = []\ntest_accuracy_lst = []"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5492b34-658f-48a1-aa99-7ca998ff30b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Training the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"447a32b9-c8a8-456d-afb7-05953f53804c"}}},{"cell_type":"code","source":["for batch_size in [500, 200]:\n    for lr in [1e-5, 1e-3, 1e-1]:\n        for wd in [0, 1e-4, 1e-2]:\n            print(f'============= Batch size {batch_size}, Learning rate {lr}, Weight Decay {wd} =============')\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n            for i in range(iteration): #tqdm(range(iteration)):\n                abort_iter = False\n                print(f'Iteration {i}', end=' ')\n                try:\n                    for df in [train_data, test_data]:\n                        \n                        # Sample the data without replacment from train/test set\n                        batch = df.sample(fraction=batch_size/total)\n                        imgs_nm1,imgs_nm2,lab = list(zip(*batch.rdd.map(lambda x : [x[0],x[1],x[2]]).collect()))\n                        imgs1=[conv(x)[0][0] for x in imgs_nm1]\n                        imgs2=[conv(x)[0][0] for x in imgs_nm2]\n\n                        label = torch.tensor(list(map(int,lab)),dtype=torch.float32).reshape(-1,1).to(device)\n                        tens1 = torch.tensor([eval(val) for val in imgs1],dtype=torch.float32)\n                        tens2 = torch.tensor([eval(val) for val in imgs2],dtype=torch.float32)\n                        ar = torch.hstack((tens1,tens2)).to(device)\n                        if df==test_data:\n                            model.eval()\n                        else:\n                            model.train()\n                        y_hat = model(ar).to(device)\n                        loss = criterion(y_hat, label)\n                        y_thresh = y_hat > 0.5\n                        accuracy = torch.mean((y_thresh == label).float())\n                        if df==train_data:\n                            train_loss_lst.append(loss.item())\n                            train_accuracy_lst.append(accuracy)\n                            optimizer.zero_grad()\n                            loss.backward()\n                            optimizer.step()\n                            print(f'training loss = {loss.item():.4f}, training accuracy = {accuracy:.4f}', end=' ')\n                        else:\n                            test_loss_lst.append(loss.item())\n                            test_accuracy_lst.append(accuracy)\n                            print(f'val loss = {loss.item():.4f}, val accuracy = {accuracy:.4f}')\n                    if i > 4:\n                        if train_accuracy_lst[-4:] == sorted(train_accuracy_lst[-4:], reverse=True):\n                            print('-------------------- Early Stopping Triggered --------------------')\n                            break\n                except ValueError:\n                        print('---------- iteration aborted ----------')\n\n                        pass\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"087dd3e3-1c5e-44a7-8bb0-d19eae6eb5f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">============= Batch size 500, Learning rate 1e-05, Weight Decay 0 =============\nIteration 0 training loss = 0.7117, training accuracy = 0.5107 val loss = 0.7231, val accuracy = 0.5584\nIteration 1 training loss = 0.6781, training accuracy = 0.5248 val loss = 0.6893, val accuracy = 0.5096\nIteration 2 training loss = 0.6457, training accuracy = 0.5138 val loss = 0.6831, val accuracy = 0.4955\nIteration 3 training loss = 0.6298, training accuracy = 0.5974 val loss = 0.6737, val accuracy = 0.4904\nIteration 4 training loss = 0.6388, training accuracy = 0.5938 val loss = 0.6673, val accuracy = 0.4430\nIteration 5 training loss = 0.6550, training accuracy = 0.5860 val loss = 0.6493, val accuracy = 0.5377\nIteration 6 training loss = 0.6169, training accuracy = 0.6232 val loss = 0.6555, val accuracy = 0.5872\nIteration 7 training loss = 0.6290, training accuracy = 0.5921 val loss = 0.6379, val accuracy = 0.6275\nIteration 8 training loss = 0.6055, training accuracy = 0.5780 val loss = 0.6188, val accuracy = 0.6602\nIteration 9 training loss = 0.5815, training accuracy = 0.6265 val loss = 0.6465, val accuracy = 0.6429\nIteration 10 training loss = 0.6012, training accuracy = 0.6103 val loss = 0.5881, val accuracy = 0.6635\nIteration 11 training loss = 0.6093, training accuracy = 0.6235 val loss = 0.6344, val accuracy = 0.6263\nIteration 12 training loss = 0.5863, training accuracy = 0.6391 val loss = 0.6359, val accuracy = 0.6531\nIteration 13 training loss = 0.5653, training accuracy = 0.6754 val loss = 0.6325, val accuracy = 0.6667\nIteration 14 training loss = 0.5729, training accuracy = 0.6600 val loss = 0.6151, val accuracy = 0.6475\nIteration 15 training loss = 0.5950, training accuracy = 0.6597 val loss = 0.5904, val accuracy = 0.6667\nIteration 16 training loss = 0.5742, training accuracy = 0.6986 val loss = 0.5305, val accuracy = 0.7917\nIteration 17 training loss = 0.5482, training accuracy = 0.7294 val loss = 0.5407, val accuracy = 0.7723\nIteration 18 training loss = 0.5245, training accuracy = 0.7304 val loss = 0.5274, val accuracy = 0.7327\nIteration 19 training loss = 0.5640, training accuracy = 0.7216 val loss = 0.5370, val accuracy = 0.7000\n============= Batch size 500, Learning rate 1e-05, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.5873, training accuracy = 0.7086 val loss = 0.5583, val accuracy = 0.6176\nIteration 1 training loss = 0.5597, training accuracy = 0.7313 val loss = 0.5977, val accuracy = 0.6495\nIteration 2 training loss = 0.5778, training accuracy = 0.7035 val loss = 0.6147, val accuracy = 0.6566\nIteration 3 training loss = 0.5585, training accuracy = 0.7278 val loss = 0.5839, val accuracy = 0.6484\nIteration 4 training loss = 0.5530, training accuracy = 0.6896 val loss = 0.5998, val accuracy = 0.5872\nIteration 5 training loss = 0.5561, training accuracy = 0.6955 val loss = 0.5536, val accuracy = 0.7500\nIteration 6 training loss = 0.5558, training accuracy = 0.7298 val loss = 0.5932, val accuracy = 0.6837\nIteration 7 training loss = 0.5717, training accuracy = 0.6905 val loss = 0.5778, val accuracy = 0.6875\nIteration 8 training loss = 0.5330, training accuracy = 0.7067 val loss = 0.5380, val accuracy = 0.7355\nIteration 9 training loss = 0.5469, training accuracy = 0.7196 val loss = 0.5191, val accuracy = 0.7157\nIteration 10 training loss = 0.5524, training accuracy = 0.7204 val loss = 0.5456, val accuracy = 0.7373\nIteration 11 training loss = 0.5359, training accuracy = 0.7094 val loss = 0.5678, val accuracy = 0.6768\nIteration 12 training loss = 0.5932, training accuracy = 0.6675 val loss = 0.5005, val accuracy = 0.7653\nIteration 13 training loss = 0.5555, training accuracy = 0.7332 val loss = 0.5359, val accuracy = 0.7614\nIteration 14 training loss = 0.5422, training accuracy = 0.7306 val loss = 0.5691, val accuracy = 0.6882\nIteration 15 training loss = 0.5259, training accuracy = 0.7180 val loss = 0.5611, val accuracy = 0.7236\nIteration 16 training loss = 0.5244, training accuracy = 0.7445 val loss = 0.5106, val accuracy = 0.7229\nIteration 17 training loss = 0.4830, training accuracy = 0.7444 val loss = 0.4749, val accuracy = 0.7835\nIteration 18 training loss = 0.5399, training accuracy = 0.7024 val loss = 0.5246, val accuracy = 0.6500\nIteration 19 training loss = 0.5559, training accuracy = 0.7153 val loss = 0.5411, val accuracy = 0.7766\n============= Batch size 500, Learning rate 1e-05, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5270, training accuracy = 0.7121 val loss = 0.4809, val accuracy = 0.7642\nIteration 1 training loss = 0.5230, training accuracy = 0.7212 val loss = 0.5413, val accuracy = 0.7586\nIteration 2 training loss = 0.5275, training accuracy = 0.7330 val loss = 0.5153, val accuracy = 0.7527\nIteration 3 training loss = 0.4744, training accuracy = 0.7537 val loss = 0.5709, val accuracy = 0.7353\nIteration 4 training loss = 0.5339, training accuracy = 0.7418 val loss = 0.5048, val accuracy = 0.7755\nIteration 5 training loss = 0.5188, training accuracy = 0.7518 val loss = 0.5091, val accuracy = 0.7778\nIteration 6 training loss = 0.5374, training accuracy = 0.7119 val loss = 0.5547, val accuracy = 0.7234\nIteration 7 training loss = 0.5314, training accuracy = 0.7416 val loss = 0.5243, val accuracy = 0.8081\nIteration 8 training loss = 0.5449, training accuracy = 0.7229 val loss = 0.5662, val accuracy = 0.7234\nIteration 9 training loss = 0.5199, training accuracy = 0.7480 val loss = 0.5420, val accuracy = 0.6522\nIteration 10 training loss = 0.5123, training accuracy = 0.7343 val loss = 0.5063, val accuracy = 0.7471\nIteration 11 training loss = 0.5251, training accuracy = 0.6878 val loss = 0.4736, val accuracy = 0.7667\nIteration 12 training loss = 0.5266, training accuracy = 0.7322 val loss = 0.5189, val accuracy = 0.7521\nIteration 13 training loss = 0.5587, training accuracy = 0.6960 val loss = 0.5191, val accuracy = 0.7500\nIteration 14 training loss = 0.5179, training accuracy = 0.7579 val loss = 0.5410, val accuracy = 0.6977\nIteration 15 training loss = 0.5079, training accuracy = 0.6995 val loss = 0.5906, val accuracy = 0.6600\nIteration 16 training loss = 0.5105, training accuracy = 0.7538 val loss = 0.5336, val accuracy = 0.7117\nIteration 17 training loss = 0.5547, training accuracy = 0.7266 val loss = 0.4962, val accuracy = 0.7093\nIteration 18 training loss = 0.5399, training accuracy = 0.7260 val loss = 0.5079, val accuracy = 0.7071\nIteration 19 training loss = 0.4937, training accuracy = 0.7579 val loss = 0.5272, val accuracy = 0.7292\n============= Batch size 500, Learning rate 0.001, Weight Decay 0 =============\nIteration 0 training loss = 0.5026, training accuracy = 0.7480 val loss = 4.9047, val accuracy = 0.5667\nIteration 1 training loss = 2.4582, training accuracy = 0.4901 val loss = 16.4791, val accuracy = 0.5051\nIteration 2 training loss = 2.4110, training accuracy = 0.5271 val loss = 6.7846, val accuracy = 0.4587\nIteration 3 training loss = 1.4082, training accuracy = 0.5242 val loss = 3.7316, val accuracy = 0.4950\nIteration 4 training loss = 0.8817, training accuracy = 0.5807 val loss = 2.5888, val accuracy = 0.5392\nIteration 5 training loss = 0.7177, training accuracy = 0.5881 val loss = 1.3961, val accuracy = 0.6186\nIteration 6 training loss = 0.6650, training accuracy = 0.6253 val loss = 1.9943, val accuracy = 0.5182\nIteration 7 training loss = 0.7200, training accuracy = 0.6124 val loss = 2.7823, val accuracy = 0.5248\nIteration 8 training loss = 0.6785, training accuracy = 0.6190 val loss = 2.7865, val accuracy = 0.5340\nIteration 9 training loss = 0.6472, training accuracy = 0.6562 val loss = 2.3453, val accuracy = 0.5612\nIteration 10 training loss = 0.6742, training accuracy = 0.5610 val loss = 2.3438, val accuracy = 0.5051\nIteration 11 training loss = 0.6552, training accuracy = 0.5854 val loss = 1.5638, val accuracy = 0.6506\nIteration 12 training loss = 0.6701, training accuracy = 0.5468 val loss = 1.5596, val accuracy = 0.5376\nIteration 13 training loss = 0.6600, training accuracy = 0.5774 val loss = 1.5376, val accuracy = 0.5155\nIteration 14 training loss = 0.6213, training accuracy = 0.6331 val loss = 1.2144, val accuracy = 0.5816\nIteration 15 training loss = 0.6449, training accuracy = 0.6167 val loss = 0.8918, val accuracy = 0.5248\nIteration 16 training loss = 0.6486, training accuracy = 0.5827 val loss = 0.7258, val accuracy = 0.6067\nIteration 17 training loss = 0.6312, training accuracy = 0.6295 val loss = 0.6903, val accuracy = 0.6538\nIteration 18 training loss = 0.6138, training accuracy = 0.6346 val loss = 0.6457, val accuracy = 0.6429\nIteration 19 training loss = 0.6271, training accuracy = 0.6209 val loss = 0.7233, val accuracy = 0.5663\n============= Batch size 500, Learning rate 0.001, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.6025, training accuracy = 0.6310 val loss = 1.0548, val accuracy = 0.5904\nIteration 1 training loss = 0.7648, training accuracy = 0.5704 val loss = 0.9989, val accuracy = 0.5565\nIteration 2 training loss = 0.6896, training accuracy = 0.5764 val loss = 0.9920, val accuracy = 0.5200\nIteration 3 training loss = 0.6295, training accuracy = 0.6049 val loss = 1.0802, val accuracy = 0.4898\nIteration 4 training loss = 0.6464, training accuracy = 0.6089 val loss = 0.7904, val accuracy = 0.5810\nIteration 5 training loss = 0.6109, training accuracy = 0.6436 val loss = 0.5802, val accuracy = 0.7556\nIteration 6 training loss = 0.6538, training accuracy = 0.6317 val loss = 0.6450, val accuracy = 0.5682\nIteration 7 training loss = 0.6205, training accuracy = 0.6714 val loss = 0.5807, val accuracy = 0.7333\nIteration 8 training loss = 0.5958, training accuracy = 0.6543 val loss = 0.6611, val accuracy = 0.6000\nIteration 9 training loss = 0.5828, training accuracy = 0.6683 val loss = 0.5737, val accuracy = 0.6042\nIteration 10 training loss = 0.5406, training accuracy = 0.7294 val loss = 0.6331, val accuracy = 0.6514\nIteration 11 training loss = 0.5652, training accuracy = 0.7160 val loss = 0.7280, val accuracy = 0.5510\nIteration 12 training loss = 0.5668, training accuracy = 0.7027 val loss = 0.7634, val accuracy = 0.5294\nIteration 13 training loss = 0.5837, training accuracy = 0.6944 val loss = 0.7267, val accuracy = 0.5876\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.001, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5781, training accuracy = 0.6461 val loss = 0.6300, val accuracy = 0.5984\nIteration 1 training loss = 0.5710, training accuracy = 0.7105 val loss = 0.6111, val accuracy = 0.6633\nIteration 2 training loss = 0.5942, training accuracy = 0.7014 val loss = 0.6222, val accuracy = 0.6889\nIteration 3 training loss = 0.5713, training accuracy = 0.7322 val loss = 0.6060, val accuracy = 0.6827\nIteration 4 training loss = 0.5604, training accuracy = 0.7165 val loss = 0.6243, val accuracy = 0.7019\nIteration 5 training loss = 0.5445, training accuracy = 0.6891 val loss = 0.5928, val accuracy = 0.7113\nIteration 6 training loss = 0.5588, training accuracy = 0.7000 val loss = 0.6840, val accuracy = 0.6379\nIteration 7 training loss = 0.5889, training accuracy = 0.7014 val loss = 0.7424, val accuracy = 0.5524\nIteration 8 training loss = 0.6219, training accuracy = 0.6878 val loss = 0.7164, val accuracy = 0.5914\nIteration 9 training loss = 0.5705, training accuracy = 0.6965 val loss = 0.9009, val accuracy = 0.5062\nIteration 10 training loss = 0.5709, training accuracy = 0.6948 val loss = 0.7929, val accuracy = 0.5686\nIteration 11 training loss = 0.5766, training accuracy = 0.7400 val loss = 0.8876, val accuracy = 0.4628\nIteration 12 training loss = 0.5740, training accuracy = 0.6895 val loss = 0.7440, val accuracy = 0.5586\nIteration 13 training loss = 0.7048, training accuracy = 0.6830 val loss = 0.7456, val accuracy = 0.5347\nIteration 14 training loss = 0.6128, training accuracy = 0.6626 val loss = 0.8095, val accuracy = 0.4757\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.1, Weight Decay 0 =============\nIteration 0 training loss = 0.6092, training accuracy = 0.6591 val loss = 3707.6909, val accuracy = 0.5114\nIteration 1 training loss = 7.6978, training accuracy = 0.5385 val loss = 1263.7527, val accuracy = 0.4954\nIteration 2 training loss = 23.7979, training accuracy = 0.5088 val loss = 625.4261, val accuracy = 0.4444\nIteration 3 training loss = 10.9591, training accuracy = 0.5193 val loss = 466.6731, val accuracy = 0.5233\nIteration 4 training loss = 5.7595, training accuracy = 0.5266 val loss = 88.4751, val accuracy = 0.4860\nIteration 5 training loss = 5.3445, training accuracy = 0.5000 val loss = 163.4808, val accuracy = 0.5217\nIteration 6 training loss = 8.1828, training accuracy = 0.5025 val loss = 126.9414, val accuracy = 0.5714\nIteration 7 training loss = 5.4685, training accuracy = 0.4804 val loss = 69.3612, val accuracy = 0.5619\nIteration 8 training loss = 3.1652, training accuracy = 0.5101 val loss = 27.0964, val accuracy = 0.5842\nIteration 9 training loss = 4.0627, training accuracy = 0.5318 val loss = 13.9170, val accuracy = 0.5269\nIteration 10 training loss = 3.6755, training accuracy = 0.5381 val loss = 19.5819, val accuracy = 0.4800\nIteration 11 training loss = 2.0332, training accuracy = 0.6099 val loss = 21.2954, val accuracy = 0.5529\nIteration 12 training loss = 1.6335, training accuracy = 0.6277 val loss = 21.7780, val accuracy = 0.4779\nIteration 13 training loss = 1.7196, training accuracy = 0.6110 val loss = 18.7621, val accuracy = 0.5321\nIteration 14 training loss = 2.0366, training accuracy = 0.5375 val loss = 8.5557, val accuracy = 0.5349\nIteration 15 training loss = 1.5332, training accuracy = 0.5172 val loss = 4.3305, val accuracy = 0.4825\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.1, Weight Decay 0.0001 =============\nIteration 0 training loss = 1.0188, training accuracy = 0.5763 val loss = 8.0364, val accuracy = 0.5392\nIteration 1 training loss = 5.0734, training accuracy = 0.4769 val loss = 0.7678, val accuracy = 0.6019\nIteration 2 training loss = 1.1069, training accuracy = 0.5369 val loss = 0.7452, val accuracy = 0.5000\nIteration 3 training loss = 1.8249, training accuracy = 0.5013 val loss = 0.7936, val accuracy = 0.4468\nIteration 4 training loss = 1.3124, training accuracy = 0.5073 val loss = 0.7094, val accuracy = 0.4957\nIteration 5 training loss = 0.9056, training accuracy = 0.4719 val loss = 0.6910, val accuracy = 0.5217\nIteration 6 training loss = 0.8237, training accuracy = 0.5356 val loss = 0.6952, val accuracy = 0.4112\nIteration 7 training loss = 0.7204, training accuracy = 0.5299 val loss = 0.6934, val accuracy = 0.5769\nIteration 8 training loss = 0.7087, training accuracy = 0.5473 val loss = 0.6989, val accuracy = 0.5133\nIteration 9 training loss = 0.6856, training accuracy = 0.5346 val loss = 0.9753, val accuracy = 0.4239\nIteration 10 training loss = 0.9184, training accuracy = 0.4824 val loss = 0.6962, val accuracy = 0.4842\nIteration 11 training loss = 0.6763, training accuracy = 0.5688 val loss = 0.7044, val accuracy = 0.5143\nIteration 12 training loss = 0.6897, training accuracy = 0.5417 val loss = 0.7111, val accuracy = 0.5408\nIteration 13 training loss = 0.6559, training accuracy = 0.5544 val loss = 0.6958, val accuracy = 0.4639\nIteration 14 training loss = 0.6712, training accuracy = 0.5464 val loss = 0.6959, val accuracy = 0.4636\nIteration 15 training loss = 0.6716, training accuracy = 0.6203 val loss = 0.7134, val accuracy = 0.5234\nIteration 16 training loss = 0.6565, training accuracy = 0.5906 val loss = 0.7570, val accuracy = 0.3571\nIteration 17 training loss = 0.6369, training accuracy = 0.5619 val loss = 0.7099, val accuracy = 0.4259\nIteration 18 training loss = 0.6287, training accuracy = 0.6399 val loss = 0.6914, val accuracy = 0.4712\nIteration 19 training loss = 0.6569, training accuracy = 0.6106 val loss = 0.6950, val accuracy = 0.5377\n============= Batch size 500, Learning rate 0.1, Weight Decay 0.01 =============\nIteration 0 training loss = 0.6809, training accuracy = 0.6015 val loss = 0.7616, val accuracy = 0.4444\nIteration 1 training loss = 0.8173, training accuracy = 0.4962 val loss = 0.7201, val accuracy = 0.5048\nIteration 2 training loss = 0.9467, training accuracy = 0.5000 val loss = 0.6872, val accuracy = 0.5851\nIteration 3 training loss = 0.8499, training accuracy = 0.4631 val loss = 0.6932, val accuracy = 0.4048\nIteration 4 training loss = 0.8278, training accuracy = 0.4780 val loss = 0.6997, val accuracy = 0.5612\nIteration 5 training loss = 0.7824, training accuracy = 0.5300 val loss = 0.7931, val accuracy = 0.5000\nIteration 6 training loss = 0.9126, training accuracy = 0.5000 val loss = 0.8698, val accuracy = 0.4624\nIteration 7 training loss = 1.1160, training accuracy = 0.4885 val loss = 0.8621, val accuracy = 0.4854\nIteration 8 training loss = 0.9845, training accuracy = 0.4577 val loss = 0.6950, val accuracy = 0.4583\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0 =============\nIteration 0 training loss = 0.7870, training accuracy = 0.5294 val loss = 0.7559, val accuracy = 0.6222\nIteration 1 training loss = 0.8061, training accuracy = 0.4586 val loss = 0.7102, val accuracy = 0.5000\nIteration 2 training loss = 0.7806, training accuracy = 0.5181 val loss = 0.6904, val accuracy = 0.4468\nIteration 3 training loss = 0.7873, training accuracy = 0.4830 val loss = 0.7264, val accuracy = 0.5455\nIteration 4 training loss = 0.7791, training accuracy = 0.5170 val loss = 0.7458, val accuracy = 0.6000\nIteration 5 training loss = 0.7974, training accuracy = 0.4242 val loss = 0.6922, val accuracy = 0.4524\nIteration 6 training loss = 0.7749, training accuracy = 0.5057 val loss = 0.6902, val accuracy = 0.4474\nIteration 7 training loss = 0.7571, training accuracy = 0.4581 val loss = 0.6523, val accuracy = 0.3409\nIteration 8 training loss = 0.8042, training accuracy = 0.3939 val loss = 0.7296, val accuracy = 0.5588\nIteration 9 training loss = 0.8058, training accuracy = 0.4852 val loss = 0.6994, val accuracy = 0.4737\nIteration 10 training loss = 0.7423, training accuracy = 0.5220 val loss = 0.7256, val accuracy = 0.5490\nIteration 11 training loss = 0.7993, training accuracy = 0.4663 val loss = 0.6792, val accuracy = 0.4167\nIteration 12 training loss = 0.7146, training accuracy = 0.4966 val loss = 0.7273, val accuracy = 0.5556\nIteration 13 training loss = 0.7468, training accuracy = 0.5584 val loss = 0.7820, val accuracy = 0.7143\nIteration 14 training loss = 0.7979, training accuracy = 0.4676 val loss = 0.7259, val accuracy = 0.5526\nIteration 15 training loss = 0.7867, training accuracy = 0.4337 val loss = 0.6922, val accuracy = 0.4545\nIteration 16 training loss = 0.7804, training accuracy = 0.5556 val loss = 0.7180, val accuracy = 0.5306\nIteration 17 training loss = 0.7141, training accuracy = 0.4788 val loss = 0.7115, val accuracy = 0.5128\nIteration 18 training loss = 0.7908, training accuracy = 0.4610 val loss = 0.7073, val accuracy = 0.5000\nIteration 19 training loss = 0.7461, training accuracy = 0.5278 val loss = 0.7189, val accuracy = 0.5333\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.8049, training accuracy = 0.4901 val loss = 0.6899, val accuracy = 0.4500\nIteration 1 training loss = 0.7879, training accuracy = 0.4929 val loss = 0.7029, val accuracy = 0.4878\nIteration 2 training loss = 0.7513, training accuracy = 0.5094 val loss = 0.7584, val accuracy = 0.6552\nIteration 3 training loss = 0.7564, training accuracy = 0.5636 val loss = 0.7481, val accuracy = 0.6250\nIteration 4 training loss = 0.7727, training accuracy = 0.5455 val loss = 0.7214, val accuracy = 0.5455\nIteration 5 training loss = 0.7337, training accuracy = 0.4860 val loss = 0.7214, val accuracy = 0.5455\nIteration 6 training loss = 0.8177, training accuracy = 0.4467 val loss = 0.7211, val accuracy = 0.5455\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0.01 =============\nIteration 0 training loss = 0.7151, training accuracy = 0.5724 val loss = 0.6916, val accuracy = 0.4545\nIteration 1 training loss = 0.7696, training accuracy = 0.5491 val loss = 0.6952, val accuracy = 0.4651\nIteration 2 training loss = 0.7388, training accuracy = 0.5714 val loss = 0.7104, val accuracy = 0.5122\nIteration 3 training loss = 0.7123, training accuracy = 0.5748 val loss = 0.7150, val accuracy = 0.5278\nIteration 4 training loss = 0.7195, training accuracy = 0.5813 val loss = 0.7525, val accuracy = 0.6486\nIteration 5 training loss = 0.7370, training accuracy = 0.5480 val loss = 0.6796, val accuracy = 0.4194\nIteration 6 training loss = 0.7968, training accuracy = 0.5183 val loss = 0.6939, val accuracy = 0.4634\nIteration 7 training loss = 0.7380, training accuracy = 0.5172 val loss = 0.6980, val accuracy = 0.4737\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.001, Weight Decay 0 =============\nIteration 0 training loss = 0.7253, training accuracy = 0.5223 val loss = 0.7087, val accuracy = 0.5366\nIteration 1 training loss = 0.6863, training accuracy = 0.6243 val loss = 0.6882, val accuracy = 0.4444\nIteration 2 training loss = 0.6210, training accuracy = 0.6369 val loss = 0.6959, val accuracy = 0.5417\nIteration 3 training loss = 0.6645, training accuracy = 0.5895 val loss = 0.6924, val accuracy = 0.6250\nIteration 4 training loss = 0.6633, training accuracy = 0.5915 val loss = 0.6926, val accuracy = 0.5000\nIteration 5 training loss = 0.6519, training accuracy = 0.6140 val loss = 0.7049, val accuracy = 0.4091\nIteration 6 training loss = 0.6594, training accuracy = 0.6387 val loss = 0.6858, val accuracy = 0.5455\nIteration 7 training loss = 0.6817, training accuracy = 0.6792 val loss = 0.6774, val accuracy = 0.5833\nIteration 8 training loss = 0.5854, training accuracy = 0.7453 val loss = 0.7041, val accuracy = 0.4839\nIteration 9 training loss = 0.5681, training accuracy = 0.6282 val loss = 0.7107, val accuracy = 0.4773\nIteration 10 training loss = 0.5841, training accuracy = 0.6456 val loss = 0.6952, val accuracy = 0.5238\nIteration 11 training loss = 0.6352, training accuracy = 0.6023 val loss = 0.7153, val accuracy = 0.4762\nIteration 12 training loss = 0.5708, training accuracy = 0.6646 val loss = 0.7119, val accuracy = 0.4783\nIteration 13 training loss = 0.6378, training accuracy = 0.5988 val loss = 0.6989, val accuracy = 0.5161\nIteration 14 training loss = 0.5895, training accuracy = 0.6188 val loss = 0.7113, val accuracy = 0.4634\nIteration 15 training loss = 0.5661, training accuracy = 0.6415 val loss = 0.6824, val accuracy = 0.5556\nIteration 16 training loss = 0.6540, training accuracy = 0.5806 val loss = 0.7468, val accuracy = 0.3235\nIteration 17 training loss = 0.5873, training accuracy = 0.6369 val loss = 0.6981, val accuracy = 0.4643\nIteration 18 training loss = 0.5931, training accuracy = 0.7006 val loss = 0.6923, val accuracy = 0.4762\nIteration 19 training loss = 0.6002, training accuracy = 0.6875 val loss = 0.6891, val accuracy = 0.5098\n============= Batch size 200, Learning rate 0.001, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.5709, training accuracy = 0.6835 val loss = 0.6921, val accuracy = 0.4583\nIteration 1 training loss = 0.5820, training accuracy = 0.6648 val loss = 0.7186, val accuracy = 0.4038\nIteration 2 training loss = 0.6529, training accuracy = 0.6069 val loss = 0.6989, val accuracy = 0.4583\nIteration 3 training loss = 0.5959, training accuracy = 0.6836 val loss = 0.6800, val accuracy = 0.5116\nIteration 4 training loss = 0.5928, training accuracy = 0.6938 val loss = 0.6711, val accuracy = 0.5745\nIteration 5 training loss = 0.5532, training accuracy = 0.7007 val loss = 0.6471, val accuracy = 0.6250\nIteration 6 training loss = 0.5913, training accuracy = 0.6560 val loss = 0.6910, val accuracy = 0.4571\nIteration 7 training loss = 0.6498, training accuracy = 0.6690 val loss = 0.6505, val accuracy = 0.5833\nIteration 8 training loss = 0.5443, training accuracy = 0.6886 val loss = 0.6685, val accuracy = 0.5000\nIteration 9 training loss = 0.5816, training accuracy = 0.6752 val loss = 0.6622, val accuracy = 0.4909\nIteration 10 training loss = 0.6204, training accuracy = 0.6164 val loss = 0.6878, val accuracy = 0.3404\nIteration 11 training loss = 0.5407, training accuracy = 0.7088 val loss = 0.6134, val accuracy = 0.6977\nIteration 12 training loss = 0.5965, training accuracy = 0.6806 val loss = 0.6189, val accuracy = 0.5000\nIteration 13 training loss = 0.5389, training accuracy = 0.7007 val loss = 0.6212, val accuracy = 0.5758\nIteration 14 training loss = 0.6224, training accuracy = 0.6667 val loss = 0.6270, val accuracy = 0.5676\nIteration 15 training loss = 0.5509, training accuracy = 0.7200 val loss = 0.6510, val accuracy = 0.4722\nIteration 16 training loss = 0.5397, training accuracy = 0.7337 val loss = 0.6529, val accuracy = 0.3947\nIteration 17 training loss = 0.5584, training accuracy = 0.7126 val loss = 0.6441, val accuracy = 0.4390\nIteration 18 training loss = 0.5597, training accuracy = 0.7232 val loss = 0.6946, val accuracy = 0.2941\nIteration 19 training loss = 0.5105, training accuracy = 0.7597 val loss = 0.5987, val accuracy = 0.5600\n============= Batch size 200, Learning rate 0.001, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5359, training accuracy = 0.7484 val loss = 0.6442, val accuracy = 0.5238\nIteration 1 training loss = 0.5670, training accuracy = 0.7093 val loss = 0.5987, val accuracy = 0.5349\nIteration 2 training loss = 0.6190, training accuracy = 0.6768 val loss = 0.6590, val accuracy = 0.3793\nIteration 3 training loss = 0.4837, training accuracy = 0.7500 val loss = 0.6044, val accuracy = 0.5405\nIteration 4 training loss = 0.6212, training accuracy = 0.7047 val loss = 0.6120, val accuracy = 0.5000\nIteration 5 training loss = 0.4905, training accuracy = 0.7785 val loss = 0.6371, val accuracy = 0.4884\nIteration 6 training loss = 0.5371, training accuracy = 0.7724 val loss = 0.6019, val accuracy = 0.5455\nIteration 7 training loss = 0.5454, training accuracy = 0.7219 val loss = 0.6368, val accuracy = 0.5128\nIteration 8 training loss = 0.5610, training accuracy = 0.6800 val loss = 0.6550, val accuracy = 0.4902\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0 =============\nIteration 0 training loss = 0.5474, training accuracy = 0.7151 val loss = 44.4647, val accuracy = 0.4898\nIteration 1 training loss = 6.0127, training accuracy = 0.5576 val loss = 169.8606, val accuracy = 0.6500\nIteration 2 training loss = 1.4507, training accuracy = 0.5139 val loss = 57.8387, val accuracy = 0.5000\nIteration 3 training loss = 2.0397, training accuracy = 0.4650 val loss = 20.8087, val accuracy = 0.4762\nIteration 4 training loss = 1.3645, training accuracy = 0.5032 val loss = 28.3409, val accuracy = 0.3714\nIteration 5 training loss = 0.7606, training accuracy = 0.5733 val loss = 12.6160, val accuracy = 0.6364\nIteration 6 training loss = 0.9992, training accuracy = 0.5161 val loss = 8.8533, val accuracy = 0.5333\nIteration 7 training loss = 0.9156, training accuracy = 0.5161 val loss = 1.7031, val accuracy = 0.5667\nIteration 8 training loss = 0.8784, training accuracy = 0.5029 val loss = 1.9410, val accuracy = 0.5455\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.7006, training accuracy = 0.5422 val loss = 7.0311, val accuracy = 0.4615\nIteration 1 training loss = 3.0153, training accuracy = 0.4778 val loss = 13.1103, val accuracy = 0.5122\nIteration 2 training loss = 1.7935, training accuracy = 0.5338 val loss = 8.4746, val accuracy = 0.2727\nIteration 3 training loss = 1.4563, training accuracy = 0.5892 val loss = 1.9507, val accuracy = 0.6190\nIteration 4 training loss = 0.9416, training accuracy = 0.5930 val loss = 2.7364, val accuracy = 0.5000\nIteration 5 training loss = 1.1594, training accuracy = 0.4367 val loss = 4.3077, val accuracy = 0.5610\nIteration 6 training loss = 0.8855, training accuracy = 0.5159 val loss = 0.9728, val accuracy = 0.5833\nIteration 7 training loss = 0.8351, training accuracy = 0.5762 val loss = 1.0939, val accuracy = 0.5476\nIteration 8 training loss = 1.1977, training accuracy = 0.4737 val loss = 0.8541, val accuracy = 0.4872\nIteration 9 training loss = 0.8465, training accuracy = 0.4588 val loss = 0.7127, val accuracy = 0.5319\nIteration 10 training loss = 0.8306, training accuracy = 0.5509 val loss = 0.9478, val accuracy = 0.5556\nIteration 11 training loss = 0.7503, training accuracy = 0.4903 val loss = 0.9207, val accuracy = 0.5610\nIteration 12 training loss = 0.6784, training accuracy = 0.5724 val loss = 0.8612, val accuracy = 0.5312\nIteration 13 training loss = 0.6573, training accuracy = 0.5706 val loss = 0.8099, val accuracy = 0.4750\nIteration 14 training loss = 0.6738, training accuracy = 0.5608 val loss = 0.6642, val accuracy = 0.5294\nIteration 15 training loss = 0.7458, training accuracy = 0.5570 val loss = 0.6768, val accuracy = 0.5263\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0.01 =============\nIteration 0 training loss = 0.6918, training accuracy = 0.5497 val loss = 0.7879, val accuracy = 0.5455\nIteration 1 training loss = 1.0911, training accuracy = 0.5127 val loss = 1.4044, val accuracy = 0.4595\nIteration 2 training loss = 1.6300, training accuracy = 0.4696 val loss = 0.7085, val accuracy = 0.4902\nIteration 3 training loss = 0.6970, training accuracy = 0.5924 val loss = 0.7070, val accuracy = 0.5429\nIteration 4 training loss = 0.8295, training accuracy = 0.5490 val loss = 1.4702, val accuracy = 0.4889\nIteration 5 training loss = 1.7633, training accuracy = 0.4667 val loss = 0.6230, val accuracy = 0.6889\nIteration 6 training loss = 0.8051, training accuracy = 0.5541 val loss = 0.7137, val accuracy = 0.4667\nIteration 7 training loss = 1.0473, training accuracy = 0.5070 val loss = 1.1821, val accuracy = 0.4667\nIteration 8 training loss = 1.5836, training accuracy = 0.5355 val loss = 0.7617, val accuracy = 0.4103\nIteration 9 training loss = 1.3081, training accuracy = 0.4551 val loss = 0.6851, val accuracy = 0.5676\nIteration 10 training loss = 0.8152, training accuracy = 0.5210 val loss = 1.1216, val accuracy = 0.4634\nIteration 11 training loss = 0.9622, training accuracy = 0.5541 val loss = 0.7050, val accuracy = 0.3871\nIteration 12 training loss = 0.6935, training accuracy = 0.4809 val loss = 0.7281, val accuracy = 0.5517\nIteration 13 training loss = 1.0040, training accuracy = 0.4744 val loss = 0.6945, val accuracy = 0.4130\nIteration 14 training loss = 1.0156, training accuracy = 0.5357 val loss = 0.6959, val accuracy = 0.4706\nIteration 15 training loss = 0.7788, training accuracy = 0.5059 val loss = 0.6948, val accuracy = 0.4808\nIteration 16 training loss = 0.6991, training accuracy = 0.5000 val loss = 0.7033, val accuracy = 0.3947\nIteration 17 training loss = 0.6913, training accuracy = 0.5064 val loss = 0.6839, val accuracy = 0.5676\nIteration 18 training loss = 0.7988, training accuracy = 0.5333 val loss = 0.7018, val accuracy = 0.4906\nIteration 19 training loss = 0.7755, training accuracy = 0.4901 val loss = 0.7046, val accuracy = 0.4250\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">============= Batch size 500, Learning rate 1e-05, Weight Decay 0 =============\nIteration 0 training loss = 0.7117, training accuracy = 0.5107 val loss = 0.7231, val accuracy = 0.5584\nIteration 1 training loss = 0.6781, training accuracy = 0.5248 val loss = 0.6893, val accuracy = 0.5096\nIteration 2 training loss = 0.6457, training accuracy = 0.5138 val loss = 0.6831, val accuracy = 0.4955\nIteration 3 training loss = 0.6298, training accuracy = 0.5974 val loss = 0.6737, val accuracy = 0.4904\nIteration 4 training loss = 0.6388, training accuracy = 0.5938 val loss = 0.6673, val accuracy = 0.4430\nIteration 5 training loss = 0.6550, training accuracy = 0.5860 val loss = 0.6493, val accuracy = 0.5377\nIteration 6 training loss = 0.6169, training accuracy = 0.6232 val loss = 0.6555, val accuracy = 0.5872\nIteration 7 training loss = 0.6290, training accuracy = 0.5921 val loss = 0.6379, val accuracy = 0.6275\nIteration 8 training loss = 0.6055, training accuracy = 0.5780 val loss = 0.6188, val accuracy = 0.6602\nIteration 9 training loss = 0.5815, training accuracy = 0.6265 val loss = 0.6465, val accuracy = 0.6429\nIteration 10 training loss = 0.6012, training accuracy = 0.6103 val loss = 0.5881, val accuracy = 0.6635\nIteration 11 training loss = 0.6093, training accuracy = 0.6235 val loss = 0.6344, val accuracy = 0.6263\nIteration 12 training loss = 0.5863, training accuracy = 0.6391 val loss = 0.6359, val accuracy = 0.6531\nIteration 13 training loss = 0.5653, training accuracy = 0.6754 val loss = 0.6325, val accuracy = 0.6667\nIteration 14 training loss = 0.5729, training accuracy = 0.6600 val loss = 0.6151, val accuracy = 0.6475\nIteration 15 training loss = 0.5950, training accuracy = 0.6597 val loss = 0.5904, val accuracy = 0.6667\nIteration 16 training loss = 0.5742, training accuracy = 0.6986 val loss = 0.5305, val accuracy = 0.7917\nIteration 17 training loss = 0.5482, training accuracy = 0.7294 val loss = 0.5407, val accuracy = 0.7723\nIteration 18 training loss = 0.5245, training accuracy = 0.7304 val loss = 0.5274, val accuracy = 0.7327\nIteration 19 training loss = 0.5640, training accuracy = 0.7216 val loss = 0.5370, val accuracy = 0.7000\n============= Batch size 500, Learning rate 1e-05, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.5873, training accuracy = 0.7086 val loss = 0.5583, val accuracy = 0.6176\nIteration 1 training loss = 0.5597, training accuracy = 0.7313 val loss = 0.5977, val accuracy = 0.6495\nIteration 2 training loss = 0.5778, training accuracy = 0.7035 val loss = 0.6147, val accuracy = 0.6566\nIteration 3 training loss = 0.5585, training accuracy = 0.7278 val loss = 0.5839, val accuracy = 0.6484\nIteration 4 training loss = 0.5530, training accuracy = 0.6896 val loss = 0.5998, val accuracy = 0.5872\nIteration 5 training loss = 0.5561, training accuracy = 0.6955 val loss = 0.5536, val accuracy = 0.7500\nIteration 6 training loss = 0.5558, training accuracy = 0.7298 val loss = 0.5932, val accuracy = 0.6837\nIteration 7 training loss = 0.5717, training accuracy = 0.6905 val loss = 0.5778, val accuracy = 0.6875\nIteration 8 training loss = 0.5330, training accuracy = 0.7067 val loss = 0.5380, val accuracy = 0.7355\nIteration 9 training loss = 0.5469, training accuracy = 0.7196 val loss = 0.5191, val accuracy = 0.7157\nIteration 10 training loss = 0.5524, training accuracy = 0.7204 val loss = 0.5456, val accuracy = 0.7373\nIteration 11 training loss = 0.5359, training accuracy = 0.7094 val loss = 0.5678, val accuracy = 0.6768\nIteration 12 training loss = 0.5932, training accuracy = 0.6675 val loss = 0.5005, val accuracy = 0.7653\nIteration 13 training loss = 0.5555, training accuracy = 0.7332 val loss = 0.5359, val accuracy = 0.7614\nIteration 14 training loss = 0.5422, training accuracy = 0.7306 val loss = 0.5691, val accuracy = 0.6882\nIteration 15 training loss = 0.5259, training accuracy = 0.7180 val loss = 0.5611, val accuracy = 0.7236\nIteration 16 training loss = 0.5244, training accuracy = 0.7445 val loss = 0.5106, val accuracy = 0.7229\nIteration 17 training loss = 0.4830, training accuracy = 0.7444 val loss = 0.4749, val accuracy = 0.7835\nIteration 18 training loss = 0.5399, training accuracy = 0.7024 val loss = 0.5246, val accuracy = 0.6500\nIteration 19 training loss = 0.5559, training accuracy = 0.7153 val loss = 0.5411, val accuracy = 0.7766\n============= Batch size 500, Learning rate 1e-05, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5270, training accuracy = 0.7121 val loss = 0.4809, val accuracy = 0.7642\nIteration 1 training loss = 0.5230, training accuracy = 0.7212 val loss = 0.5413, val accuracy = 0.7586\nIteration 2 training loss = 0.5275, training accuracy = 0.7330 val loss = 0.5153, val accuracy = 0.7527\nIteration 3 training loss = 0.4744, training accuracy = 0.7537 val loss = 0.5709, val accuracy = 0.7353\nIteration 4 training loss = 0.5339, training accuracy = 0.7418 val loss = 0.5048, val accuracy = 0.7755\nIteration 5 training loss = 0.5188, training accuracy = 0.7518 val loss = 0.5091, val accuracy = 0.7778\nIteration 6 training loss = 0.5374, training accuracy = 0.7119 val loss = 0.5547, val accuracy = 0.7234\nIteration 7 training loss = 0.5314, training accuracy = 0.7416 val loss = 0.5243, val accuracy = 0.8081\nIteration 8 training loss = 0.5449, training accuracy = 0.7229 val loss = 0.5662, val accuracy = 0.7234\nIteration 9 training loss = 0.5199, training accuracy = 0.7480 val loss = 0.5420, val accuracy = 0.6522\nIteration 10 training loss = 0.5123, training accuracy = 0.7343 val loss = 0.5063, val accuracy = 0.7471\nIteration 11 training loss = 0.5251, training accuracy = 0.6878 val loss = 0.4736, val accuracy = 0.7667\nIteration 12 training loss = 0.5266, training accuracy = 0.7322 val loss = 0.5189, val accuracy = 0.7521\nIteration 13 training loss = 0.5587, training accuracy = 0.6960 val loss = 0.5191, val accuracy = 0.7500\nIteration 14 training loss = 0.5179, training accuracy = 0.7579 val loss = 0.5410, val accuracy = 0.6977\nIteration 15 training loss = 0.5079, training accuracy = 0.6995 val loss = 0.5906, val accuracy = 0.6600\nIteration 16 training loss = 0.5105, training accuracy = 0.7538 val loss = 0.5336, val accuracy = 0.7117\nIteration 17 training loss = 0.5547, training accuracy = 0.7266 val loss = 0.4962, val accuracy = 0.7093\nIteration 18 training loss = 0.5399, training accuracy = 0.7260 val loss = 0.5079, val accuracy = 0.7071\nIteration 19 training loss = 0.4937, training accuracy = 0.7579 val loss = 0.5272, val accuracy = 0.7292\n============= Batch size 500, Learning rate 0.001, Weight Decay 0 =============\nIteration 0 training loss = 0.5026, training accuracy = 0.7480 val loss = 4.9047, val accuracy = 0.5667\nIteration 1 training loss = 2.4582, training accuracy = 0.4901 val loss = 16.4791, val accuracy = 0.5051\nIteration 2 training loss = 2.4110, training accuracy = 0.5271 val loss = 6.7846, val accuracy = 0.4587\nIteration 3 training loss = 1.4082, training accuracy = 0.5242 val loss = 3.7316, val accuracy = 0.4950\nIteration 4 training loss = 0.8817, training accuracy = 0.5807 val loss = 2.5888, val accuracy = 0.5392\nIteration 5 training loss = 0.7177, training accuracy = 0.5881 val loss = 1.3961, val accuracy = 0.6186\nIteration 6 training loss = 0.6650, training accuracy = 0.6253 val loss = 1.9943, val accuracy = 0.5182\nIteration 7 training loss = 0.7200, training accuracy = 0.6124 val loss = 2.7823, val accuracy = 0.5248\nIteration 8 training loss = 0.6785, training accuracy = 0.6190 val loss = 2.7865, val accuracy = 0.5340\nIteration 9 training loss = 0.6472, training accuracy = 0.6562 val loss = 2.3453, val accuracy = 0.5612\nIteration 10 training loss = 0.6742, training accuracy = 0.5610 val loss = 2.3438, val accuracy = 0.5051\nIteration 11 training loss = 0.6552, training accuracy = 0.5854 val loss = 1.5638, val accuracy = 0.6506\nIteration 12 training loss = 0.6701, training accuracy = 0.5468 val loss = 1.5596, val accuracy = 0.5376\nIteration 13 training loss = 0.6600, training accuracy = 0.5774 val loss = 1.5376, val accuracy = 0.5155\nIteration 14 training loss = 0.6213, training accuracy = 0.6331 val loss = 1.2144, val accuracy = 0.5816\nIteration 15 training loss = 0.6449, training accuracy = 0.6167 val loss = 0.8918, val accuracy = 0.5248\nIteration 16 training loss = 0.6486, training accuracy = 0.5827 val loss = 0.7258, val accuracy = 0.6067\nIteration 17 training loss = 0.6312, training accuracy = 0.6295 val loss = 0.6903, val accuracy = 0.6538\nIteration 18 training loss = 0.6138, training accuracy = 0.6346 val loss = 0.6457, val accuracy = 0.6429\nIteration 19 training loss = 0.6271, training accuracy = 0.6209 val loss = 0.7233, val accuracy = 0.5663\n============= Batch size 500, Learning rate 0.001, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.6025, training accuracy = 0.6310 val loss = 1.0548, val accuracy = 0.5904\nIteration 1 training loss = 0.7648, training accuracy = 0.5704 val loss = 0.9989, val accuracy = 0.5565\nIteration 2 training loss = 0.6896, training accuracy = 0.5764 val loss = 0.9920, val accuracy = 0.5200\nIteration 3 training loss = 0.6295, training accuracy = 0.6049 val loss = 1.0802, val accuracy = 0.4898\nIteration 4 training loss = 0.6464, training accuracy = 0.6089 val loss = 0.7904, val accuracy = 0.5810\nIteration 5 training loss = 0.6109, training accuracy = 0.6436 val loss = 0.5802, val accuracy = 0.7556\nIteration 6 training loss = 0.6538, training accuracy = 0.6317 val loss = 0.6450, val accuracy = 0.5682\nIteration 7 training loss = 0.6205, training accuracy = 0.6714 val loss = 0.5807, val accuracy = 0.7333\nIteration 8 training loss = 0.5958, training accuracy = 0.6543 val loss = 0.6611, val accuracy = 0.6000\nIteration 9 training loss = 0.5828, training accuracy = 0.6683 val loss = 0.5737, val accuracy = 0.6042\nIteration 10 training loss = 0.5406, training accuracy = 0.7294 val loss = 0.6331, val accuracy = 0.6514\nIteration 11 training loss = 0.5652, training accuracy = 0.7160 val loss = 0.7280, val accuracy = 0.5510\nIteration 12 training loss = 0.5668, training accuracy = 0.7027 val loss = 0.7634, val accuracy = 0.5294\nIteration 13 training loss = 0.5837, training accuracy = 0.6944 val loss = 0.7267, val accuracy = 0.5876\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.001, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5781, training accuracy = 0.6461 val loss = 0.6300, val accuracy = 0.5984\nIteration 1 training loss = 0.5710, training accuracy = 0.7105 val loss = 0.6111, val accuracy = 0.6633\nIteration 2 training loss = 0.5942, training accuracy = 0.7014 val loss = 0.6222, val accuracy = 0.6889\nIteration 3 training loss = 0.5713, training accuracy = 0.7322 val loss = 0.6060, val accuracy = 0.6827\nIteration 4 training loss = 0.5604, training accuracy = 0.7165 val loss = 0.6243, val accuracy = 0.7019\nIteration 5 training loss = 0.5445, training accuracy = 0.6891 val loss = 0.5928, val accuracy = 0.7113\nIteration 6 training loss = 0.5588, training accuracy = 0.7000 val loss = 0.6840, val accuracy = 0.6379\nIteration 7 training loss = 0.5889, training accuracy = 0.7014 val loss = 0.7424, val accuracy = 0.5524\nIteration 8 training loss = 0.6219, training accuracy = 0.6878 val loss = 0.7164, val accuracy = 0.5914\nIteration 9 training loss = 0.5705, training accuracy = 0.6965 val loss = 0.9009, val accuracy = 0.5062\nIteration 10 training loss = 0.5709, training accuracy = 0.6948 val loss = 0.7929, val accuracy = 0.5686\nIteration 11 training loss = 0.5766, training accuracy = 0.7400 val loss = 0.8876, val accuracy = 0.4628\nIteration 12 training loss = 0.5740, training accuracy = 0.6895 val loss = 0.7440, val accuracy = 0.5586\nIteration 13 training loss = 0.7048, training accuracy = 0.6830 val loss = 0.7456, val accuracy = 0.5347\nIteration 14 training loss = 0.6128, training accuracy = 0.6626 val loss = 0.8095, val accuracy = 0.4757\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.1, Weight Decay 0 =============\nIteration 0 training loss = 0.6092, training accuracy = 0.6591 val loss = 3707.6909, val accuracy = 0.5114\nIteration 1 training loss = 7.6978, training accuracy = 0.5385 val loss = 1263.7527, val accuracy = 0.4954\nIteration 2 training loss = 23.7979, training accuracy = 0.5088 val loss = 625.4261, val accuracy = 0.4444\nIteration 3 training loss = 10.9591, training accuracy = 0.5193 val loss = 466.6731, val accuracy = 0.5233\nIteration 4 training loss = 5.7595, training accuracy = 0.5266 val loss = 88.4751, val accuracy = 0.4860\nIteration 5 training loss = 5.3445, training accuracy = 0.5000 val loss = 163.4808, val accuracy = 0.5217\nIteration 6 training loss = 8.1828, training accuracy = 0.5025 val loss = 126.9414, val accuracy = 0.5714\nIteration 7 training loss = 5.4685, training accuracy = 0.4804 val loss = 69.3612, val accuracy = 0.5619\nIteration 8 training loss = 3.1652, training accuracy = 0.5101 val loss = 27.0964, val accuracy = 0.5842\nIteration 9 training loss = 4.0627, training accuracy = 0.5318 val loss = 13.9170, val accuracy = 0.5269\nIteration 10 training loss = 3.6755, training accuracy = 0.5381 val loss = 19.5819, val accuracy = 0.4800\nIteration 11 training loss = 2.0332, training accuracy = 0.6099 val loss = 21.2954, val accuracy = 0.5529\nIteration 12 training loss = 1.6335, training accuracy = 0.6277 val loss = 21.7780, val accuracy = 0.4779\nIteration 13 training loss = 1.7196, training accuracy = 0.6110 val loss = 18.7621, val accuracy = 0.5321\nIteration 14 training loss = 2.0366, training accuracy = 0.5375 val loss = 8.5557, val accuracy = 0.5349\nIteration 15 training loss = 1.5332, training accuracy = 0.5172 val loss = 4.3305, val accuracy = 0.4825\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 500, Learning rate 0.1, Weight Decay 0.0001 =============\nIteration 0 training loss = 1.0188, training accuracy = 0.5763 val loss = 8.0364, val accuracy = 0.5392\nIteration 1 training loss = 5.0734, training accuracy = 0.4769 val loss = 0.7678, val accuracy = 0.6019\nIteration 2 training loss = 1.1069, training accuracy = 0.5369 val loss = 0.7452, val accuracy = 0.5000\nIteration 3 training loss = 1.8249, training accuracy = 0.5013 val loss = 0.7936, val accuracy = 0.4468\nIteration 4 training loss = 1.3124, training accuracy = 0.5073 val loss = 0.7094, val accuracy = 0.4957\nIteration 5 training loss = 0.9056, training accuracy = 0.4719 val loss = 0.6910, val accuracy = 0.5217\nIteration 6 training loss = 0.8237, training accuracy = 0.5356 val loss = 0.6952, val accuracy = 0.4112\nIteration 7 training loss = 0.7204, training accuracy = 0.5299 val loss = 0.6934, val accuracy = 0.5769\nIteration 8 training loss = 0.7087, training accuracy = 0.5473 val loss = 0.6989, val accuracy = 0.5133\nIteration 9 training loss = 0.6856, training accuracy = 0.5346 val loss = 0.9753, val accuracy = 0.4239\nIteration 10 training loss = 0.9184, training accuracy = 0.4824 val loss = 0.6962, val accuracy = 0.4842\nIteration 11 training loss = 0.6763, training accuracy = 0.5688 val loss = 0.7044, val accuracy = 0.5143\nIteration 12 training loss = 0.6897, training accuracy = 0.5417 val loss = 0.7111, val accuracy = 0.5408\nIteration 13 training loss = 0.6559, training accuracy = 0.5544 val loss = 0.6958, val accuracy = 0.4639\nIteration 14 training loss = 0.6712, training accuracy = 0.5464 val loss = 0.6959, val accuracy = 0.4636\nIteration 15 training loss = 0.6716, training accuracy = 0.6203 val loss = 0.7134, val accuracy = 0.5234\nIteration 16 training loss = 0.6565, training accuracy = 0.5906 val loss = 0.7570, val accuracy = 0.3571\nIteration 17 training loss = 0.6369, training accuracy = 0.5619 val loss = 0.7099, val accuracy = 0.4259\nIteration 18 training loss = 0.6287, training accuracy = 0.6399 val loss = 0.6914, val accuracy = 0.4712\nIteration 19 training loss = 0.6569, training accuracy = 0.6106 val loss = 0.6950, val accuracy = 0.5377\n============= Batch size 500, Learning rate 0.1, Weight Decay 0.01 =============\nIteration 0 training loss = 0.6809, training accuracy = 0.6015 val loss = 0.7616, val accuracy = 0.4444\nIteration 1 training loss = 0.8173, training accuracy = 0.4962 val loss = 0.7201, val accuracy = 0.5048\nIteration 2 training loss = 0.9467, training accuracy = 0.5000 val loss = 0.6872, val accuracy = 0.5851\nIteration 3 training loss = 0.8499, training accuracy = 0.4631 val loss = 0.6932, val accuracy = 0.4048\nIteration 4 training loss = 0.8278, training accuracy = 0.4780 val loss = 0.6997, val accuracy = 0.5612\nIteration 5 training loss = 0.7824, training accuracy = 0.5300 val loss = 0.7931, val accuracy = 0.5000\nIteration 6 training loss = 0.9126, training accuracy = 0.5000 val loss = 0.8698, val accuracy = 0.4624\nIteration 7 training loss = 1.1160, training accuracy = 0.4885 val loss = 0.8621, val accuracy = 0.4854\nIteration 8 training loss = 0.9845, training accuracy = 0.4577 val loss = 0.6950, val accuracy = 0.4583\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0 =============\nIteration 0 training loss = 0.7870, training accuracy = 0.5294 val loss = 0.7559, val accuracy = 0.6222\nIteration 1 training loss = 0.8061, training accuracy = 0.4586 val loss = 0.7102, val accuracy = 0.5000\nIteration 2 training loss = 0.7806, training accuracy = 0.5181 val loss = 0.6904, val accuracy = 0.4468\nIteration 3 training loss = 0.7873, training accuracy = 0.4830 val loss = 0.7264, val accuracy = 0.5455\nIteration 4 training loss = 0.7791, training accuracy = 0.5170 val loss = 0.7458, val accuracy = 0.6000\nIteration 5 training loss = 0.7974, training accuracy = 0.4242 val loss = 0.6922, val accuracy = 0.4524\nIteration 6 training loss = 0.7749, training accuracy = 0.5057 val loss = 0.6902, val accuracy = 0.4474\nIteration 7 training loss = 0.7571, training accuracy = 0.4581 val loss = 0.6523, val accuracy = 0.3409\nIteration 8 training loss = 0.8042, training accuracy = 0.3939 val loss = 0.7296, val accuracy = 0.5588\nIteration 9 training loss = 0.8058, training accuracy = 0.4852 val loss = 0.6994, val accuracy = 0.4737\nIteration 10 training loss = 0.7423, training accuracy = 0.5220 val loss = 0.7256, val accuracy = 0.5490\nIteration 11 training loss = 0.7993, training accuracy = 0.4663 val loss = 0.6792, val accuracy = 0.4167\nIteration 12 training loss = 0.7146, training accuracy = 0.4966 val loss = 0.7273, val accuracy = 0.5556\nIteration 13 training loss = 0.7468, training accuracy = 0.5584 val loss = 0.7820, val accuracy = 0.7143\nIteration 14 training loss = 0.7979, training accuracy = 0.4676 val loss = 0.7259, val accuracy = 0.5526\nIteration 15 training loss = 0.7867, training accuracy = 0.4337 val loss = 0.6922, val accuracy = 0.4545\nIteration 16 training loss = 0.7804, training accuracy = 0.5556 val loss = 0.7180, val accuracy = 0.5306\nIteration 17 training loss = 0.7141, training accuracy = 0.4788 val loss = 0.7115, val accuracy = 0.5128\nIteration 18 training loss = 0.7908, training accuracy = 0.4610 val loss = 0.7073, val accuracy = 0.5000\nIteration 19 training loss = 0.7461, training accuracy = 0.5278 val loss = 0.7189, val accuracy = 0.5333\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.8049, training accuracy = 0.4901 val loss = 0.6899, val accuracy = 0.4500\nIteration 1 training loss = 0.7879, training accuracy = 0.4929 val loss = 0.7029, val accuracy = 0.4878\nIteration 2 training loss = 0.7513, training accuracy = 0.5094 val loss = 0.7584, val accuracy = 0.6552\nIteration 3 training loss = 0.7564, training accuracy = 0.5636 val loss = 0.7481, val accuracy = 0.6250\nIteration 4 training loss = 0.7727, training accuracy = 0.5455 val loss = 0.7214, val accuracy = 0.5455\nIteration 5 training loss = 0.7337, training accuracy = 0.4860 val loss = 0.7214, val accuracy = 0.5455\nIteration 6 training loss = 0.8177, training accuracy = 0.4467 val loss = 0.7211, val accuracy = 0.5455\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 1e-05, Weight Decay 0.01 =============\nIteration 0 training loss = 0.7151, training accuracy = 0.5724 val loss = 0.6916, val accuracy = 0.4545\nIteration 1 training loss = 0.7696, training accuracy = 0.5491 val loss = 0.6952, val accuracy = 0.4651\nIteration 2 training loss = 0.7388, training accuracy = 0.5714 val loss = 0.7104, val accuracy = 0.5122\nIteration 3 training loss = 0.7123, training accuracy = 0.5748 val loss = 0.7150, val accuracy = 0.5278\nIteration 4 training loss = 0.7195, training accuracy = 0.5813 val loss = 0.7525, val accuracy = 0.6486\nIteration 5 training loss = 0.7370, training accuracy = 0.5480 val loss = 0.6796, val accuracy = 0.4194\nIteration 6 training loss = 0.7968, training accuracy = 0.5183 val loss = 0.6939, val accuracy = 0.4634\nIteration 7 training loss = 0.7380, training accuracy = 0.5172 val loss = 0.6980, val accuracy = 0.4737\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.001, Weight Decay 0 =============\nIteration 0 training loss = 0.7253, training accuracy = 0.5223 val loss = 0.7087, val accuracy = 0.5366\nIteration 1 training loss = 0.6863, training accuracy = 0.6243 val loss = 0.6882, val accuracy = 0.4444\nIteration 2 training loss = 0.6210, training accuracy = 0.6369 val loss = 0.6959, val accuracy = 0.5417\nIteration 3 training loss = 0.6645, training accuracy = 0.5895 val loss = 0.6924, val accuracy = 0.6250\nIteration 4 training loss = 0.6633, training accuracy = 0.5915 val loss = 0.6926, val accuracy = 0.5000\nIteration 5 training loss = 0.6519, training accuracy = 0.6140 val loss = 0.7049, val accuracy = 0.4091\nIteration 6 training loss = 0.6594, training accuracy = 0.6387 val loss = 0.6858, val accuracy = 0.5455\nIteration 7 training loss = 0.6817, training accuracy = 0.6792 val loss = 0.6774, val accuracy = 0.5833\nIteration 8 training loss = 0.5854, training accuracy = 0.7453 val loss = 0.7041, val accuracy = 0.4839\nIteration 9 training loss = 0.5681, training accuracy = 0.6282 val loss = 0.7107, val accuracy = 0.4773\nIteration 10 training loss = 0.5841, training accuracy = 0.6456 val loss = 0.6952, val accuracy = 0.5238\nIteration 11 training loss = 0.6352, training accuracy = 0.6023 val loss = 0.7153, val accuracy = 0.4762\nIteration 12 training loss = 0.5708, training accuracy = 0.6646 val loss = 0.7119, val accuracy = 0.4783\nIteration 13 training loss = 0.6378, training accuracy = 0.5988 val loss = 0.6989, val accuracy = 0.5161\nIteration 14 training loss = 0.5895, training accuracy = 0.6188 val loss = 0.7113, val accuracy = 0.4634\nIteration 15 training loss = 0.5661, training accuracy = 0.6415 val loss = 0.6824, val accuracy = 0.5556\nIteration 16 training loss = 0.6540, training accuracy = 0.5806 val loss = 0.7468, val accuracy = 0.3235\nIteration 17 training loss = 0.5873, training accuracy = 0.6369 val loss = 0.6981, val accuracy = 0.4643\nIteration 18 training loss = 0.5931, training accuracy = 0.7006 val loss = 0.6923, val accuracy = 0.4762\nIteration 19 training loss = 0.6002, training accuracy = 0.6875 val loss = 0.6891, val accuracy = 0.5098\n============= Batch size 200, Learning rate 0.001, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.5709, training accuracy = 0.6835 val loss = 0.6921, val accuracy = 0.4583\nIteration 1 training loss = 0.5820, training accuracy = 0.6648 val loss = 0.7186, val accuracy = 0.4038\nIteration 2 training loss = 0.6529, training accuracy = 0.6069 val loss = 0.6989, val accuracy = 0.4583\nIteration 3 training loss = 0.5959, training accuracy = 0.6836 val loss = 0.6800, val accuracy = 0.5116\nIteration 4 training loss = 0.5928, training accuracy = 0.6938 val loss = 0.6711, val accuracy = 0.5745\nIteration 5 training loss = 0.5532, training accuracy = 0.7007 val loss = 0.6471, val accuracy = 0.6250\nIteration 6 training loss = 0.5913, training accuracy = 0.6560 val loss = 0.6910, val accuracy = 0.4571\nIteration 7 training loss = 0.6498, training accuracy = 0.6690 val loss = 0.6505, val accuracy = 0.5833\nIteration 8 training loss = 0.5443, training accuracy = 0.6886 val loss = 0.6685, val accuracy = 0.5000\nIteration 9 training loss = 0.5816, training accuracy = 0.6752 val loss = 0.6622, val accuracy = 0.4909\nIteration 10 training loss = 0.6204, training accuracy = 0.6164 val loss = 0.6878, val accuracy = 0.3404\nIteration 11 training loss = 0.5407, training accuracy = 0.7088 val loss = 0.6134, val accuracy = 0.6977\nIteration 12 training loss = 0.5965, training accuracy = 0.6806 val loss = 0.6189, val accuracy = 0.5000\nIteration 13 training loss = 0.5389, training accuracy = 0.7007 val loss = 0.6212, val accuracy = 0.5758\nIteration 14 training loss = 0.6224, training accuracy = 0.6667 val loss = 0.6270, val accuracy = 0.5676\nIteration 15 training loss = 0.5509, training accuracy = 0.7200 val loss = 0.6510, val accuracy = 0.4722\nIteration 16 training loss = 0.5397, training accuracy = 0.7337 val loss = 0.6529, val accuracy = 0.3947\nIteration 17 training loss = 0.5584, training accuracy = 0.7126 val loss = 0.6441, val accuracy = 0.4390\nIteration 18 training loss = 0.5597, training accuracy = 0.7232 val loss = 0.6946, val accuracy = 0.2941\nIteration 19 training loss = 0.5105, training accuracy = 0.7597 val loss = 0.5987, val accuracy = 0.5600\n============= Batch size 200, Learning rate 0.001, Weight Decay 0.01 =============\nIteration 0 training loss = 0.5359, training accuracy = 0.7484 val loss = 0.6442, val accuracy = 0.5238\nIteration 1 training loss = 0.5670, training accuracy = 0.7093 val loss = 0.5987, val accuracy = 0.5349\nIteration 2 training loss = 0.6190, training accuracy = 0.6768 val loss = 0.6590, val accuracy = 0.3793\nIteration 3 training loss = 0.4837, training accuracy = 0.7500 val loss = 0.6044, val accuracy = 0.5405\nIteration 4 training loss = 0.6212, training accuracy = 0.7047 val loss = 0.6120, val accuracy = 0.5000\nIteration 5 training loss = 0.4905, training accuracy = 0.7785 val loss = 0.6371, val accuracy = 0.4884\nIteration 6 training loss = 0.5371, training accuracy = 0.7724 val loss = 0.6019, val accuracy = 0.5455\nIteration 7 training loss = 0.5454, training accuracy = 0.7219 val loss = 0.6368, val accuracy = 0.5128\nIteration 8 training loss = 0.5610, training accuracy = 0.6800 val loss = 0.6550, val accuracy = 0.4902\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0 =============\nIteration 0 training loss = 0.5474, training accuracy = 0.7151 val loss = 44.4647, val accuracy = 0.4898\nIteration 1 training loss = 6.0127, training accuracy = 0.5576 val loss = 169.8606, val accuracy = 0.6500\nIteration 2 training loss = 1.4507, training accuracy = 0.5139 val loss = 57.8387, val accuracy = 0.5000\nIteration 3 training loss = 2.0397, training accuracy = 0.4650 val loss = 20.8087, val accuracy = 0.4762\nIteration 4 training loss = 1.3645, training accuracy = 0.5032 val loss = 28.3409, val accuracy = 0.3714\nIteration 5 training loss = 0.7606, training accuracy = 0.5733 val loss = 12.6160, val accuracy = 0.6364\nIteration 6 training loss = 0.9992, training accuracy = 0.5161 val loss = 8.8533, val accuracy = 0.5333\nIteration 7 training loss = 0.9156, training accuracy = 0.5161 val loss = 1.7031, val accuracy = 0.5667\nIteration 8 training loss = 0.8784, training accuracy = 0.5029 val loss = 1.9410, val accuracy = 0.5455\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0.0001 =============\nIteration 0 training loss = 0.7006, training accuracy = 0.5422 val loss = 7.0311, val accuracy = 0.4615\nIteration 1 training loss = 3.0153, training accuracy = 0.4778 val loss = 13.1103, val accuracy = 0.5122\nIteration 2 training loss = 1.7935, training accuracy = 0.5338 val loss = 8.4746, val accuracy = 0.2727\nIteration 3 training loss = 1.4563, training accuracy = 0.5892 val loss = 1.9507, val accuracy = 0.6190\nIteration 4 training loss = 0.9416, training accuracy = 0.5930 val loss = 2.7364, val accuracy = 0.5000\nIteration 5 training loss = 1.1594, training accuracy = 0.4367 val loss = 4.3077, val accuracy = 0.5610\nIteration 6 training loss = 0.8855, training accuracy = 0.5159 val loss = 0.9728, val accuracy = 0.5833\nIteration 7 training loss = 0.8351, training accuracy = 0.5762 val loss = 1.0939, val accuracy = 0.5476\nIteration 8 training loss = 1.1977, training accuracy = 0.4737 val loss = 0.8541, val accuracy = 0.4872\nIteration 9 training loss = 0.8465, training accuracy = 0.4588 val loss = 0.7127, val accuracy = 0.5319\nIteration 10 training loss = 0.8306, training accuracy = 0.5509 val loss = 0.9478, val accuracy = 0.5556\nIteration 11 training loss = 0.7503, training accuracy = 0.4903 val loss = 0.9207, val accuracy = 0.5610\nIteration 12 training loss = 0.6784, training accuracy = 0.5724 val loss = 0.8612, val accuracy = 0.5312\nIteration 13 training loss = 0.6573, training accuracy = 0.5706 val loss = 0.8099, val accuracy = 0.4750\nIteration 14 training loss = 0.6738, training accuracy = 0.5608 val loss = 0.6642, val accuracy = 0.5294\nIteration 15 training loss = 0.7458, training accuracy = 0.5570 val loss = 0.6768, val accuracy = 0.5263\n-------------------- Early Stopping Triggered --------------------\n============= Batch size 200, Learning rate 0.1, Weight Decay 0.01 =============\nIteration 0 training loss = 0.6918, training accuracy = 0.5497 val loss = 0.7879, val accuracy = 0.5455\nIteration 1 training loss = 1.0911, training accuracy = 0.5127 val loss = 1.4044, val accuracy = 0.4595\nIteration 2 training loss = 1.6300, training accuracy = 0.4696 val loss = 0.7085, val accuracy = 0.4902\nIteration 3 training loss = 0.6970, training accuracy = 0.5924 val loss = 0.7070, val accuracy = 0.5429\nIteration 4 training loss = 0.8295, training accuracy = 0.5490 val loss = 1.4702, val accuracy = 0.4889\nIteration 5 training loss = 1.7633, training accuracy = 0.4667 val loss = 0.6230, val accuracy = 0.6889\nIteration 6 training loss = 0.8051, training accuracy = 0.5541 val loss = 0.7137, val accuracy = 0.4667\nIteration 7 training loss = 1.0473, training accuracy = 0.5070 val loss = 1.1821, val accuracy = 0.4667\nIteration 8 training loss = 1.5836, training accuracy = 0.5355 val loss = 0.7617, val accuracy = 0.4103\nIteration 9 training loss = 1.3081, training accuracy = 0.4551 val loss = 0.6851, val accuracy = 0.5676\nIteration 10 training loss = 0.8152, training accuracy = 0.5210 val loss = 1.1216, val accuracy = 0.4634\nIteration 11 training loss = 0.9622, training accuracy = 0.5541 val loss = 0.7050, val accuracy = 0.3871\nIteration 12 training loss = 0.6935, training accuracy = 0.4809 val loss = 0.7281, val accuracy = 0.5517\nIteration 13 training loss = 1.0040, training accuracy = 0.4744 val loss = 0.6945, val accuracy = 0.4130\nIteration 14 training loss = 1.0156, training accuracy = 0.5357 val loss = 0.6959, val accuracy = 0.4706\nIteration 15 training loss = 0.7788, training accuracy = 0.5059 val loss = 0.6948, val accuracy = 0.4808\nIteration 16 training loss = 0.6991, training accuracy = 0.5000 val loss = 0.7033, val accuracy = 0.3947\nIteration 17 training loss = 0.6913, training accuracy = 0.5064 val loss = 0.6839, val accuracy = 0.5676\nIteration 18 training loss = 0.7988, training accuracy = 0.5333 val loss = 0.7018, val accuracy = 0.4906\nIteration 19 training loss = 0.7755, training accuracy = 0.4901 val loss = 0.7046, val accuracy = 0.4250\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a59762b8-a992-4a39-9f7e-0115712bfa85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be1db14f-a666-45ee-a5a7-799b6997918d"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.11","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Images","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3164413804758318}},"nbformat":4,"nbformat_minor":0}
